# fly.toml app configuration file
app = "get-education-backend"
primary_region = "ord"

[build]
  dockerfile = "Dockerfile.fly"

[env]
  PORT = "8000"
  DEBUG = "false"
  APP_NAME = "Language Learning AI Backend"
  ALGORITHM = "HS256"
  ACCESS_TOKEN_EXPIRE_MINUTES = "30"
  UPLOAD_FOLDER = "uploads"
  MAX_FILE_SIZE = "10485760"

[http_service]
  internal_port = 8000
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ["app"]

  [http_service.concurrency]
    type = "connections"
    hard_limit = 25
    soft_limit = 20

[[services]]
  internal_port = 8000
  processes = ["app"]
  protocol = "tcp"
  script_checks = []

  [services.concurrency]
    hard_limit = 25
    soft_limit = 20
    type = "connections"

  [[services.ports]]
    force_https = true
    handlers = ["http"]
    port = 80

  [[services.ports]]
    handlers = ["tls", "http"]
    port = 443

  [[services.tcp_checks]]
    grace_period = "1s"
    interval = "15s"
    restart_limit = 0
    timeout = "2s"

# Health check
[[services.http_checks]]
  interval = "10s"
  grace_period = "5s"
  method = "get"
  path = "/health"
  protocol = "http"
  timeout = "2s"
  tls_skip_verify = false

# Process groups
[processes]
  app = "python -m uvicorn app.main:app --host 0.0.0.0 --port 8000"
  worker = "python -m celery -A workers.celery_app worker --loglevel=info"

# Volume for uploads
[mounts]
  source = "uploads_vol"
  destination = "/app/uploads"

# Deploy configuration - FIXED
[deploy]
  release_command = "python -m alembic upgrade head"